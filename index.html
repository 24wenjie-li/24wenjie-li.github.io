<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Wenjie Li">
    <meta property="og:title" content="Wenjie Li">
	  <meta property="og:description" content="PhD candidate, BUPT">
    <!-- <meta property="og:image" content="files/me.jpg"> -->
	  <meta property="og:url" content="https://24wenjie-li.github.io/">
	  <!-- <meta name="twitter:card" content="summary_large_image"> -->
    <title>Wenjie Li</title>
    <link rel="apple-touch-icon" href="assets/logo.jpg">
    <link rel="icon" type="image/png" href="assets/logo.jpg">
    <!-- <link rel="manifest" href="files/site.webmanifest"> -->
    <link rel="stylesheet" href="style.css">
</head>
<body>
<div class="header noselect">
    <div class="header-div content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Wenjie Li (ÊùéÊñáÊù∞)</h1>
            </div>
            <div class="header-subtitle">
              Ph.D. candidate, BUPT
            </div>
            <div class="header-links">
                <a class="btn" href="mailto:lewj2408@gmail.com">Email</a> /
                <a class="btn" href="https://scholar.google.com.hk/citations?user=8_-tznoAAAAJ&hl=zh-CN">Google Scholar</a> /
                <a class="btn" href="https://github.com/24wenjie-li">GitHub</a> /
                <a class="btn" href="assets/wechat.jpg">Wechat</a> /
                <a class="btn" href="https://www.linkedin.com/in/%E6%96%87%E6%9D%B0-%E6%9D%8E-4ab508272/">LinkedIn</a> /
                <a class="btn" href="publication.html">Publication</a>
            </div>
        </div>
    </div>
</div>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
            I am a Ph.D. candidate at <a href="https://www.bupt.edu.cn/">Beijing University of Posts and Telecommunications</a>, advised by Prof. <a href="https://zhanyuma.cn/" target="_blank">Zhanyu Ma</a> and co-advised by <a href="https://gh-home.github.io/">Heng Guo</a>. 
            Before starting my Ph.D. phase, I received master's degree at <a href="https://www.njupt.edu.cn/">Nanjing University of Posts and Telecommunications</a> in 2023, advised by Prof. <a href="https://guangweigao.github.io/" target="_blank">Guangwei Gao</a>.
            I have been fortunate to intern at <a href="https://www.megvii.com/">Megvii Technology</a>. <br/><br/>
            My research interests include Deep Learning üß† and Computer Vision ü§ñ, particularly focusing on Low-Level Vision üì∑, model lightweighting üöÄ, and Artificial Intelligence Generated Content (AIGC) üíª.
        </p>
    </div>
    <div>

    
    
    <!-- News -->
    <div id="news">
      <h2 class="noselect">News</h2>
      <div class="news-container">
          <ul class="news-list noselect">
              <li class="news-item">2025/12 -- One paper <a href="https://arxiv.org/pdf/2503.10043">FourierSR</a> got accepted by TIP 2025 <span class="bold">(CCF-A)</span>.</li>
              <li class="news-item">2025/11 -- One <a href="https://arxiv.org/pdf/2309.15490">Survey paper</a> of face restoration got accepted by ACM CSUR 2025 <span class="bold">(IF=28.0)</span>.</li>
              <li class="news-item">2025/11 -- One paper <a href="">DHGM</a> got accepted by AAAI 2026 <span class="bold">(CCF-A)</span>.</li>
              <li class="news-item">2025/09 -- One paper <a href="https://arxiv.org/pdf/2510.12114">SSDiff</a> got accepted by NeurIPS 2025 <span class="bold">(CCF-A)</span>.</li>
              <li class="news-item">2025/08 -- One co-author paper <a href="https://arxiv.org/pdf/2409.00591">AMINet</a> got accepted by TSMCA 2025 <span class="bold">(CCF-B)</span>.</li>
              <li class="news-item">2025/08 -- One paper <a href="https://arxiv.org/pdf/2503.10047?">DMNet</a> got accepted by TMM 2025 <span class="bold">(THU-A, CCF-B)</span>.</li>
              <!-- <li class="news-item">2025/03 -- I have won the 7th place (7/37) in <a href="https://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/">NTIRE 2025 challenge on day and night raindrop removal for dual-focused images</a> as the team leader.</li> -->
              <li class="news-item">2025/02 -- Our paper <a href="https://arxiv.org/pdf/2207.02796">CFIN</a> was selected as <span class="bold">ESI Highly Cited Paper! (1%)</span>
              <li class="news-item">2024/09 -- One paper <a href="https://arxiv.org/pdf/2212.14181">FIWHN</a> got accepted by TMM 2024 <span class="bold">(THU-A, CCF-B)</span>.</li>
              <li class="news-item">2024/07 -- One paper <a href="https://arxiv.org/pdf/2407.19768?">WFEN</a> got accepted by ACM MM 2024 <span class="bold">(CCF-A)</span>.</li>
              <li class="news-item">2023/10 -- We have released the <a href="https://github.com/24wenjie-li/Awesome-Face-Restoration">Awesome-Face-Restoration</a>, collecting comprehensive Face Restoration methods! </li>
              <li class="news-item">2023/05 -- One paper <a href="https://arxiv.org/pdf/2207.02796">CFIN</a> got accepted by TMM 2023 <span class="bold">(THU-A, CCF-B)</span>.</li>
              <li class="news-item">2023/04 -- One co-author <a href="https://arxiv.org/pdf/2109.14335">Survey paper</a> of image super-resolution got accepted by ACM CSUR 2023 <span class="bold">(IF=16.6)</span>.</li>
              <li class="news-item">2022/04 -- One co-author paper <a href="https://guangweigao.github.io/IJCAI22/LBNet.html">LBNet</a> got accepted by IJCAI 2022 <span class="bold">(CCF-A)</span> and selected as <span class="bold">short oral</span>.</li>
              <li class="news-item">2021/12 -- One paper <a href="https://arxiv.org/abs/2112.08655">FDIWN</a> got accepted by AAAI 2022 <span class="bold">(CCF-A)</span>.</li>
          </ul>
      </div>
  </div>

    

        <h2 class="noselect">Selected Publications and Preprints</h2>
        <p>* Equal contribution. # Corresponding author. Representative papers are <span style="background-color: #fff8df">highlighted</span>.</p>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/TMS.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">Measurement-Constrained Sampling for Text-Prompted Blind Face Restoration</a><br/>
                <span class="bold">Wenjie Li</span>, Yulun Zhang, Guangwei Gao, Heng Guo, Zhanyu Ma<br/>
                <span class="italic">Arxiv:2511.14213 (<a href="https://arxiv.org/abs/2511.14213">Under Review</a>)</span>, 2025<br/>
                <a class="btn btn-red" href="https://arxiv.org/abs/2511.14213">arXiv</a> / <a class="btn" href="">code</a> / <a class="btn btn-dark" href="bibtex/TMS.txt">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/FourierSR.gif);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">FourierSR: A Fourier Token-based Plugin for Efficient Image Super-Resolution</a><br/>
                <span class="bold">Wenjie Li</span>, Heng Guo#, Yuefeng Hou, Zhanyu Ma<br/>
                <span class="italic">IEEE Transactions on Image Processing</span>, 2026<br/>
                <a class="btn btn-red" href="https://arxiv.org/pdf/2503.10043">arXiv</a> / <a class="btn" href="">code</a> / <a class="btn btn-dark" href="https://mp.weixin.qq.com/s/0VHYAC1zM024xzXR9bxRVw">ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑</a> / <a class="btn btn-dark" href="bibtex/TIP_FourierSR.txt">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/AAAI26_DHGM.gif);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">Seeing Through the Rain: Resolving High-Frequency Conflicts in Deraining and Super-Resolution via Diffusion Guidance</a><br/>
                <span class="bold">Wenjie Li</span>, Jinglei Shi, Jin Han, Heng Guo#, Zhanyu Ma<br/>
                <span class="italic">AAAI Conference on Artificial Intelligence (<a href="https://aaai.org/conference/aaai/aaai-26/">AAAI</a></span>), 2026 <br/>
                <a class="btn btn-orange" href="https://24wenjie-li.github.io/projects/WFEN/index.html">project page</a> / <a class="btn btn-red" href="https://arxiv.org/pdf/2511.12419">arXiv</a> / <a class="btn" href="https://github.com/PRIS-CV/DHGM">code</a> / <a class="btn btn-dark" href="bibtex/AAAI26_DHGM.txt">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/Survey.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://dl.acm.org/doi/10.1145/3778162">Survey on Deep Face Restoration: From Non-blind to Blind and Beyond</a><br/>
                <span class="bold">Wenjie Li</span>, Mei Wang, Kai Zhang, Juncheng Li, Xiaoming Li, Yuhang Zhang,<br/> 
                Guangwei Gao#, Zhanyu Ma<br/>
                <span class="italic">ACM Computing Surveys (<a href="https://dl.acm.org/journal/csur">ACM CSUR</a>)</span>, 2025<br/>
                <!-- <span class="italic">Arxiv:2309.15490 (<a href="">Under Review</a>)</span>, 2023<br/> -->
                <a class="btn btn-red" href="https://arxiv.org/pdf/2309.15490">arXiv</a> / <a class="btn" href="https://github.com/24wenjie-li/Awesome-Face-Restoration">code</a> / <a class="btn btn-dark" href="bibtex/CSUR25_Face_Survey.txt">bibtex</a>
            </div>
        </div>

        <div class="highlight publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/NeurIPs25_SSDiff.gif);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://neurips.cc/virtual/2025/loc/san-diego/poster/115678">Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration</a><br/>
                <span class="bold">Wenjie Li</span>, Xiangyi Wang, Heng Guo#, Guangwei Gao, Zhanyu Ma<br/>
                <span class="italic">Advances in Neural Information Processing Systems (<a href="https://neurips.cc/Conferences/2025">NeurIPS</a></span>), 2025 <br/>
                <a class="btn btn-orange" href="https://24wenjie-li.github.io/projects/SSDiff/">project page</a> / <a class="btn btn-orange" href="https://arxiv.org/pdf/2510.12114">arXiv</a> / <a class="btn" href="https://github.com/PRIS-CV/SSDiff">code</a> / <a class="btn btn-dark" href="https://mp.weixin.qq.com/s/i5Mdw3Yxx3scMg5-viiksg">ÂæÆ‰ø°ÂÖ¨‰ºóÂè∑</a> / <a class="btn btn-dark" href="bibtex/NeurIPs25_SSDiff.txt">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/TMM25_DMNet.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="">Dual-domain Modulation Network for Lightweight Image Super-Resolution</a><br/>
                <span class="bold">Wenjie Li</span>, Heng Guo#, Yuefeng Hou, Guangwei Gao, Zhanyu Ma<br/>
                <span class="italic">IEEE Transactions on Multimedia (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a></span>), 2025 <br/>
                <a class="btn btn-red" href="https://arxiv.org/pdf/2503.10047?">arXiv</a> / <a class="btn" href="https://github.com/24wenjie-li/DMNet">code</a> / <a class="btn btn-dark" href="bibtex/TMM25_DMNet.txt">bibtex</a>
            </div>
        </div>
 
        <div class="publication row clearfix">
          <div class="row-media" style="background-image: url(assets/papers/TMM24_FIWNN.png);"></div>
          <div class="row-text">
              <a class="publication-title bold" href="https://ieeexplore.ieee.org/document/10814081">Efficient Image Super-Resolution with Feature Interaction Weighted Hybrid Network</a><br/>
              <span class="bold">Wenjie Li</span>, Juncheng Li, Guangwei Gao#, Weihong Deng, Jian Yang, Chia-Wen Lin<br/>
              <span class="italic">IEEE Transactions on Multimedia (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a></span>), 2024 <br/>
              <a class="btn btn-red" href="https://arxiv.org/abs/2212.14181">arXiv</a> / <a class="btn" href="https://github.com/24wenjie-li/FIWHN">code</a> / <a class="btn btn-dark" href="bibtex/TMM24_FIWHN.txt">bibtex</a>
          </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/MM24_WFEN.gif);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://dl.acm.org/doi/10.1145/3664647.3681088">Efficient Face Super-Resolution via Wavelet-based Feature Enhancement Network</a><br/>
                <span class="bold">Wenjie Li</span>, Heng Guo#, Xuannan Liu, Kongming Liang, Jiani Hu, Zhanyu Ma, Jun Guo<br/>
                <span class="italic">ACM Multimedia (<a href="https://2024.acmmm.org/registration">ACM MM</a>)</span>, 2024<br/>
                <a class="btn btn-orange" href="https://24wenjie-li.github.io/projects/WFEN/index.html">project page</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2407.19768">arXiv</a> / <a class="btn" href="https://github.com/PRIS-CV/WFEN">code</a> / <a class="btn btn-dark" href="https://mp.weixin.qq.com/s/23ku4kp0se8Y5KxVW2gziA">CCF-ÂØºËØªÊé®Ëçê</a> / <a class="btn btn-dark" href="https://zhuanlan.zhihu.com/p/16532542052">Áü•‰πé</a> / <a class="btn btn-dark" href="bibtex/MM24_WFEN.txt">bibtex</a>
            </div>
        </div>

        <div class="highlight publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/TMM23_CFIN.gif);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://ieeexplore.ieee.org/document/10114600">Cross-receptive Focused Inference Network for Lightweight Image Super-Resolution</a><br/>
                <span class="bold">Wenjie Li</span>, Juncheng Li, Guangwei Gao#, Weihong Deng, Jian Yang, Guojun Qi<br/>
                <span class="italic">IEEE Transactions on Multimedia (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a></span>), 2023 (<span class="bold" style="color: #f09228;">ESI Highly Cited Paper üèÜ</span>)<br/>
                <a class="btn btn-orange" href="https://arxiv.org/abs/2207.02796">arXiv</a> / <a class="btn" href="https://github.com/24wenjie-li/CFIN">code</a> / <a class="btn btn-dark" href="https://mp.weixin.qq.com/s/eZp9xcurgY_SIT3d7TK09w">CCF-ÂØºËØªÊé®Ëçê</a>/ <a class="btn btn-dark" href="bibtex/TMM23_CFIN.txt">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(assets/papers/AAAI22_FDIWN.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://ojs.aaai.org/index.php/AAAI/article/view/19946">Feature Distillation Interaction Weighting Network for Lightweight Image Super-resolution</a><br/>
                Guangwei Gao*#, <span class="bold">Wenjie Li*</span>, Juncheng Li, Fei Wu, Huimin Lu, Yi Yu<br/>
                <span class="italic">AAAI Conference on Artificial Intelligence (<a href="https://aaai.org/conference/aaai/aaai-22/">AAAI</a>)</span>, 2022 <br/> 
                <a class="btn btn-red" href="https://guangweigao.github.io/AAAI22/FDIWN.html">project page</a> / <a class="btn btn-red" href="https://arxiv.org/abs/2112.08655">arXiv</a> / <a class="btn" href="https://github.com/24wenjie-li/FDIWN">code</a> / <a class="btn btn-dark" href="bibtex/AAAI22_FDIWN.txt">bibtex</a>
            </div>
        </div>

    </div>
    <!-- (<span class="bold" style="color: #f09228;">Beginning My Academic</span>) -->



    <!-- Experience -->
    <div id="experiences">
        <h2 class="noselect">Experience</h2>
        <div class="timeline noselect">
            <!-- <div class="timeline-item">
                <table>
                    <tr>
                        <td class="icon">
                            <a href="https://adobe.com/">
                            <img style="width: 125px;" src="https://www.adobe.com/home/assets/adobe_logo_red.svg" alt="Adobe"></a><br>
                            <span style="font-size: 13px;">2024/06 - 2024/11 <br> San Jose CA, USA</span>
                        </td>
                        <td class="description">
                            <h4><b>Research Scientist Intern</b></h4>
                            <p style="font-size: 13px;">Working with amazing researchers such as <a href="https://sniklaus.com/">Simon Niklaus</a>, <a href="https://likesum.github.io/">Zhihao Xia</a>, <a href="https://ztzhang.info/">Zhoutong Zhang</a> and <a href="https://people.csail.mit.edu/jiawen/">Jiawen Chen</a> (<a href="https://graphics.stanford.edu/~levoy/">Marc Levoy</a>'s team), where I was designing novel algorithms on practical denoising for in-the-wild videos and provide tech-support for <a href="https://www.adobe.com/products/premiere.html">Premiere Pro</a>.</p>
                        </td>
                    </tr>
                </table>
            </div> -->
            <div class="timeline-item">
                <table>
                    <tr>
                        <td class="icon">
                            <a href="https://en.megvii.com/">
                            <img src="assets/megvii_logo.png" alt="Megvii"></a><br>
                            <span style="font-size: 13px;">2022/05 - 2022/11 <span style="font-size: 8px;">(Engineer)</span><br> Beijing, China</span>
                        </td>
                        <td class="description">
                            <h4><b>Research Intern</b></h4>
                            <p style="font-size: 13px;">Worked with <a href="https://huggingface.co/xinsir">Qi Xin</a>, where I worked on model lightweighting techniques for 3D human pose estimation models (<a href="https://github.com/nkolot/SPIN">SPIN</a>), including knowledge distillation and model pruning.</p>
                        </td>
                    </tr>
                </table>
            </div>
        </div>

        <!-- <details class="noselect">
            <summary style="font-size: 14px;">As Conference and Workshop Committee</summary>
            <div class="timeline noselect">
                <div class="timeline-item">
                    <table>
                        <tr>
                            <td class="icon">
                                <a href="https://cvlai.net/ntire/2025/">
                                <img src="assets/experience/NTIRE2020_logo.png" alt="NTIRE"></a><br>
                                <span style="font-size: 13px;">2025/01 - 2025/06<br> Nashville TN, USA</span>
                            </td>
                            <td class="description">
                                <h4><b>Co-organizer</b></h4>
                                <p style="font-size: 13px;">
                                    Help organized the NTIRE workshop @ CVPR 2025 with <a href="https://www.cvlai.net/">Radu Timofte</a>, etc, especially track <a href="https://codalab.lisn.upsaclay.fr/competitions/21316">Efficient Burst HDR and Restoration</a>.
                                    Report at <a href="https://arxiv.org/abs/2505.12089">here</a>.
                                </p>
                            </td>
                        </tr>
                    </table>
                </div>
                <div class="timeline-item">
                    <table>
                        <tr>
                            <td class="icon">
                                <a href="https://mipi-challenge.org/MIPI2024/">
                                <img src="/assets/experience/mipi.png" alt="MIPI"></a><br>
                                <span style="font-size: 13px;">2023/12 - 2024/06<br> Seattle WA, USA</span>
                            </td>
                            <td class="description">
                                <h4><b>Co-organizer</b></h4>
                                <p style="font-size: 13px;">
                                    Organized the MIPI workshop @ CVPR 2024 with <a href="https://www.mmlab-ntu.com/person/ccloy/">Chen Change Loy</a>, <a href="https://www.jimmyren.com/">Jimmy S. Ren</a>, <a href="https://shangchenzhou.com/">Shangchen Zhou</a>, etc. 
                                    I lead the <a href="https://codalab.lisn.upsaclay.fr/competitions/17017">Few-shot RAW Image Denoising Track</a>.
                                    Report at <a href="https://openaccess.thecvf.com/content/CVPR2024W/MIPI/papers/Jin_MIPI_2024_Challenge_on_Few-shot_RAW_Image_Denoising_Methods_and_CVPRW_2024_paper.pdf">here</a>.
                                </p>
                            </td>
                        </tr>
                    </table>
                </div>
            </div>
        </details>
    </div> -->

    <!-- Awards -->
    <div>
        <h2 class="noselect">Services and Competitions</h2>
        <ul class="noselect">
            <li style="margin-bottom: 8px;">Reviewer: <a href="https://cvpr.thecvf.com/">CVPR</a>, <a href="https://2024.acmmm.org/registration">ACM MM</a>, <a href="https://aaai.org/about-aaai/">AAAI</a>, <a href="https://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=83">TIP</a>, <a href="https://cis.ieee.org/publications/t-neural-networks-and-learning-systems">TNNLS</a>, <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046">TMM</a>, <a href="https://dl.acm.org/journal/csur">ACM CSUR</a>, etc.</li>
            <li style="margin-bottom: 8px;">7th place (7/37) in <a href="https://lixinustc.github.io/CVPR-NTIRE2025-RainDrop-Competition.github.io/">Day and Night Raindrop Removal for Dual-Focused Images</a>, CVPR NTIRE 2025, serve as team leader.</li>
        </ul>
    </div>


<!-- <div>
  <h2 class="noselect">Press coverage</h2>
  I have been mentioned in various media in connection with my research. Here's a few select articles:
  <div class="row clearfix" style="margin-top: 16px;">
      <a href="https://blogs.nvidia.com/blog/national-robotics-week-2025#nicklas-hansen" class="press row-media" style="background-image: url(files/nvidia.png)"></a>
      <a href="https://generallyintelligent.com/podcast/2022-12-16-podcast-episode-25-nicklas-hansen" class="press row-media" style="background-image: url(files/generallyintelligent.png)"></a>
      <a href="https://bdtechtalks.com/2022/04/04/reinforcement-learning-td-mpc" class="press row-media" style="background-image: url(files/bdtalks.png)"></a>
      <a href="https://bair.berkeley.edu/blog/2021/02/25/ss-adaptation" class="press row-media" style="background-image: url(files/bair.png)"></a>
      <a href="https://blog.deeplearning.ai/blog/the-batch-predicting-car-crashes-profiting-from-deepfakes-piloting-drone-swarms-grading-data" class="press row-media" style="background-image: url(https://i.imgur.com/eoASzgo.png)"></a>
  </div>
</div> -->

<div class="noselect" style="margin-bottom:25px;">
  <a id="contact"></a>
  <h2>Contact</h2>
  You are welcome to contact me regarding my research! My email is <span class="bold">lewj2408@</span><span class="bold">gmail.com üì©</span>
</div>

<div id="footer-clusrmaps" style="text-align: center; margin-bottom: 15px;">
    <div id="clustrmaps-widget" style="width: 100px; margin: 0 auto; overflow: hidden;">
        <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=FrOxcKMRZRWmcADbm_-pJvbedF_Pg2wyX5ZrP3woKQw"></script>    </div>
</div>

<div class="footer noselect">
<div class="footer-content">
  &copy; 2025 Wenjie Li. This template is maintained by <a href="https://24wenjie-li.github.io/">Wenjie Li</a>, you could find the source code <a href="https://github.com/24wenjie-li/24wenjie-li.github.io/">here</a>.
<!-- </div> -->
</div>



</body>
</html>
